{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/jupyter-lab/repo/PMGT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import backoff\n",
    "import joblib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.sparse as sp\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "from pmgt.preprocessing.datasets import (\n",
    "    AmazonReviewImageDataset,\n",
    "    AmazonReviewTextDataset,\n",
    "    text_collate_fn,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "[Amazon Review Datasets](https://nijianmo.github.io/amazon/index.html)\n",
    "- Video Games\n",
    "- Toys and Games\n",
    "- Tools and Home Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/VG http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "!wget -P data/TG i://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Toys_and_Games_5.json.gz\n",
    "!wget -P data/THIi http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Tools_and_Home_Improvement_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VG Dataset\n",
    "# data_dir = \"./data/VG\"\n",
    "# filename = \"Video_Games_5.json.gz\"\n",
    "\n",
    "# TG Dataset\n",
    "data_dir = \"./data/TG\"\n",
    "filename = \"Toys_and_Games_5.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e212ddfb934d1eb70c7dcc0b816e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1828971"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open(os.path.join(data_dir, filename)) as f:\n",
    "    data = [json.loads(l.strip()) for l in tqdm(f)]\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df['reviewDateTime'] = df['unixReviewTime'].map(lambda x: datetime.fromtimestamp(x))\n",
    "df = df.sort_values(by='reviewDateTime')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507775\n",
      "1321196\n"
     ]
    }
   ],
   "source": [
    "criterion = datetime(2015, 1, 1, 9)\n",
    "df1 = df[df['reviewDateTime'] < criterion].reset_index(drop=True)\n",
    "df2 = df[df['reviewDateTime'] >= criterion].reset_index(drop=True)\n",
    "print(len(df1))\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root_path = os.path.join(data_dir, \"images\")\n",
    "os.makedirs(image_root_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49379e74e6b42da9c29ca5d6703afe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7384\n",
      "48270\n",
      "2667\n"
     ]
    }
   ],
   "source": [
    "def _giveup(e):\n",
    "    return str(e) == \"404\"\n",
    "\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (requests.exceptions.RequestException, requests.exceptions.ConnectionError),\n",
    "    max_time=30,\n",
    "    max_tries=5,\n",
    "    giveup=_giveup,\n",
    ")\n",
    "def download_image(filepath, image_url):\n",
    "    if os.path.exists(filepath):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        r = requests.get(image_url, stream=True)\n",
    "    except requests.exceptions.MissingSchema:\n",
    "        return False\n",
    "\n",
    "    if r.status_code == 404:\n",
    "        return False\n",
    "    elif r.status_code != 200:\n",
    "        raise requests.exceptions.RequestException(r.status_code)\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in r.iter_content(1024):\n",
    "            f.write(chunk)\n",
    "            \n",
    "    return True\n",
    "\n",
    "\n",
    "download_list = []\n",
    "counter = Counter()\n",
    "\n",
    "for index, row in df1[~pd.isna(df1[\"image\"])].iterrows():\n",
    "    for i, image_url in enumerate(row[\"image\"]):\n",
    "        ext = os.path.splitext(image_url)[1]\n",
    "        item_id = row[\"asin\"]\n",
    "        filepath = os.path.join(image_root_path, item_id, f\"{counter[item_id]}{ext}\")\n",
    "        counter[item_id] += 1\n",
    "        download_list.append((filepath, image_url))\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(filepath)):\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "results = Parallel(n_jobs=50, prefer=\"threads\")(\n",
    "    delayed(download_image)(f, u) for f, u in tqdm(download_list)\n",
    ")\n",
    "\n",
    "print(len(download_list))\n",
    "print(len(df1[\"asin\"].unique()))\n",
    "print(len(next(os.walk(image_root_path))[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Visual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/inceptionv4-8e4777a0.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/inceptionv4-8e4777a0.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490b20e81c4249febc44ba11fce1b6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c878d00ec246a9a93c67e4948ebea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = timm.create_model(\"inception_v4\", pretrained=True)\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)\n",
    "dataset = AmazonReviewImageDataset(\n",
    "    image_root_path, transforms=transform, item_ids=df1[\"asin\"].unique()\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=8)\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "visual_feats = []\n",
    "for batch_x in tqdm(dataloader, total=len(dataloader)):\n",
    "    batch_x = batch_x.cuda()\n",
    "    with torch.no_grad():\n",
    "        feat = model.global_pool(model.forward_features(batch_x))\n",
    "        visual_feats.append(feat.cpu())\n",
    "\n",
    "visual_feats = torch.cat(visual_feats)\n",
    "\n",
    "item_visual_feats = []\n",
    "start = 0\n",
    "for num in tqdm(dataset.num_images.values()):\n",
    "    end = start + num\n",
    "    item_visual_feats.append(visual_feats[start:end].mean(dim=0))\n",
    "    start = end\n",
    "item_visual_feats = torch.stack(item_visual_feats).numpy()\n",
    "item_mapping = np.array([item_id for item_id in dataset.num_images.keys()])\n",
    "\n",
    "np.savez(\n",
    "    os.path.join(data_dir, \"visual_feats.npz\"),\n",
    "    feats=item_visual_feats,\n",
    "    mapping=item_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = (\n",
    "    df1[~pd.isna(df1[\"reviewText\"])]\n",
    "    .groupby(\"asin\")\n",
    "    .apply(lambda r: r[\"reviewText\"].values)\n",
    ")\n",
    "review_text = review_text.to_dict()\n",
    "\n",
    "dataset = AmazonReviewTextDataset(review_text)\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=16,\n",
    "    collate_fn=partial(text_collate_fn, tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "text_feats = []\n",
    "\n",
    "for batch_x in tqdm(dataloader, total=len(dataloader)):\n",
    "    batch_x = {k: v.cuda() for k, v in batch_x.items()}\n",
    "    with torch.no_grad():\n",
    "        text_feats.append(model(**batch_x)[0][:, 0].cpu())\n",
    "\n",
    "text_feats = torch.cat(text_feats)\n",
    "\n",
    "item_textual_feats = []\n",
    "start = 0\n",
    "for num in tqdm(dataset.num_texts.values()):\n",
    "    end = start + num\n",
    "    item_textual_feats.append(text_feats[start:end].mean(dim=0))\n",
    "    start = end\n",
    "item_textual_feats = torch.stack(item_textual_feats).numpy()\n",
    "item_mapping = np.array([item_id for item_id in dataset.num_texts.keys()])\n",
    "\n",
    "np.savez(\n",
    "    os.path.join(data_dir, \"textual_feats.npz\"),\n",
    "    feats=item_textual_feats,\n",
    "    mapping=item_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Product Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22b00267b9c42e8a29364d4333a08cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f079b1c31b794fd8b755c7877f42a79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5606ce4bbb0848c19bd9f29e8002423a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7252\n",
      "88606\n"
     ]
    }
   ],
   "source": [
    "graph_data = []\n",
    "users_per_item = df1.groupby(by=\"asin\").apply(lambda r: set(r[\"reviewerID\"].unique()))\n",
    "item_ids = df1[\"asin\"].unique()\n",
    "\n",
    "item_encoder = LabelEncoder().fit(item_ids)\n",
    "user_encoder = LabelEncoder().fit(df1[\"reviewerID\"].unique())\n",
    "\n",
    "item_user_mat = sp.dok_matrix(\n",
    "    (len(item_encoder.classes_), len(user_encoder.classes_)), dtype=np.int32\n",
    ")\n",
    "\n",
    "item_to_idx = {v: i for i, v in enumerate(item_encoder.classes_)}\n",
    "user_to_idx = {v: i for i, v in enumerate(user_encoder.classes_)}\n",
    "\n",
    "for item in tqdm(item_ids):\n",
    "    item_id = item_to_idx[item]\n",
    "    user_ids = [user_to_idx[u] for u in users_per_item[item]]\n",
    "    item_user_mat[item_id, user_ids] = 1\n",
    "\n",
    "\n",
    "item_user_mat_csr = item_user_mat.tocsr()\n",
    "item_item_mat = item_user_mat_csr @ item_user_mat_csr.T\n",
    "item_item_mat.setdiag(0)\n",
    "item_item_mat.eliminate_zeros()\n",
    "\n",
    "graph_data = []\n",
    "for i, row in enumerate(tqdm(item_item_mat, total=item_item_mat.shape[0])):\n",
    "    for j, r in zip(row.indices, row.data):\n",
    "        if r >= 3:\n",
    "            graph_data.append((item_encoder.classes_[i], item_encoder.classes_[j], r))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(graph_data)\n",
    "\n",
    "for u, v, w in tqdm(G.edges.data(\"weight\")):\n",
    "    w = (np.log(w) + 1) / (np.log(np.sqrt(G.degree[u] * G.degree[v])) + 1)\n",
    "    G.edges[u, v][\"weight\"] = w\n",
    "\n",
    "nx.write_gpickle(G, os.path.join(data_dir, \"graph.gpickle\"))\n",
    "\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/TG/node_encoder']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_encoder = LabelEncoder().fit(list(G.nodes.keys()))\n",
    "joblib.dump(node_encoder, os.path.join(data_dir, \"node_encoder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['asin'].isin(G.nodes.keys())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User & Item Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/TG/item_encoder']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_encoder = LabelEncoder().fit(df3['reviewerID'].unique())\n",
    "item_encoder = LabelEncoder().fit(df3['asin'].unique())\n",
    "\n",
    "joblib.dump(user_encoder, os.path.join(data_dir, 'user_encoder'))\n",
    "joblib.dump(item_encoder, os.path.join(data_dir, 'item_encoder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(2022)\n",
    "train_df, test_df = train_test_split(df3, test_size=0.2, random_state=random_state)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "train_df.to_json(os.path.join(data_dir, 'train.json'), date_format='iso')\n",
    "test_df.to_json(os.path.join(data_dir, 'test.json'), date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Initial Feature Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_encoder = joblib.load(os.path.join(data_dir, \"node_encoder\"))\n",
    "\n",
    "with np.load(os.path.join(data_dir, \"visual_feats.npz\"), allow_pickle=True) as npz:\n",
    "    visual_feats = npz[\"feats\"]\n",
    "    visual_feats_mapping = npz[\"mapping\"]\n",
    "\n",
    "with np.load(os.path.join(data_dir, \"textual_feats.npz\"), allow_pickle=True) as npz:\n",
    "    textual_feats = npz[\"feats\"]\n",
    "    textual_feats_mapping = npz[\"mapping\"]\n",
    "\n",
    "\n",
    "def get_feat_init_emb(node_size, items, feats, item_to_idx):\n",
    "    feat_init_emb = np.empty((node_size + 2, feats.shape[1]), dtype=np.float32)\n",
    "    feat_init_emb[0] = np.zeros_like(feat_init_emb[0])  # 0 for padding\n",
    "    feat_init_emb[1] = np.zeros_like(feat_init_emb[1])  # 1 for masking\n",
    "\n",
    "    for i, item in enumerate(items, start=2):\n",
    "        if item not in item_to_idx:\n",
    "            feat_init_emb[i] = np.random.normal(size=feats.shape[1])\n",
    "        else:\n",
    "            feat_init_emb[i] = feats[item_to_idx[item]]\n",
    "\n",
    "    return feat_init_emb\n",
    "\n",
    "\n",
    "node_size = len(node_encoder.classes_)\n",
    "item_to_idx = {item: i for i, item in enumerate(visual_feats_mapping)}\n",
    "visual_init_emb = get_feat_init_emb(\n",
    "    node_size, node_encoder.classes_, visual_feats, item_to_idx\n",
    ")\n",
    "\n",
    "item_to_idx = {item: i for i, item in enumerate(textual_feats_mapping)}\n",
    "textual_init_emb = get_feat_init_emb(\n",
    "    node_size, node_encoder.classes_, textual_feats, item_to_idx\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(data_dir, \"visual_init_emb.npy\"),\n",
    "    visual_init_emb,\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(data_dir, \"textual_init_emb.npy\"),\n",
    "    textual_init_emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embedding Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_num = 16\n",
    "num_layers = 2\n",
    "emb_size = factor_num * 2 ** (num_layers - 1)\n",
    "emb_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pmgt]",
   "language": "python",
   "name": "conda-env-pmgt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
